var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#AutoDiff","page":"API","title":"AutoDiff","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [AutoDiff]","category":"page"},{"location":"api/#JITrench.AutoDiff.AbstractTensor","page":"API","title":"JITrench.AutoDiff.AbstractTensor","text":"AbstractTensor <: Variable\n\nAbstract tensor variable. Super type of Tensor and CuTensor.\n\n\n\n\n\n","category":"type"},{"location":"api/#JITrench.AutoDiff.BinaryOperator","page":"API","title":"JITrench.AutoDiff.BinaryOperator","text":"BinaryOperator\n\nDiffableFunction which takes two variables as input.\n\n\n\n\n\n","category":"type"},{"location":"api/#JITrench.AutoDiff.CPU","page":"API","title":"JITrench.AutoDiff.CPU","text":"CPU <: Device\n\nRepersents CPU device. This is used for the default device.\n\n\n\n\n\n","category":"type"},{"location":"api/#JITrench.AutoDiff.GPU","page":"API","title":"JITrench.AutoDiff.GPU","text":"GPU(idx::Int64) <: Device\n\nRepersents GPU device. idx is corresponding device index in CUDA.jl.\n\n\n\n\n\n","category":"type"},{"location":"api/#JITrench.AutoDiff.GradField","page":"API","title":"JITrench.AutoDiff.GradField","text":"GradField(inputs, output, generation)\n\nRetain the information of the function for backpropagation. All DiffableFunction must have this field.\n\n\n\n\n\n","category":"type"},{"location":"api/#JITrench.AutoDiff.NotSameDeviceError","page":"API","title":"JITrench.AutoDiff.NotSameDeviceError","text":"NotSameDeviceError <: Exception\n\nException thrown when the device of two tensors are not the same.\n\n\n\n\n\n","category":"type"},{"location":"api/#JITrench.AutoDiff.Scalar","page":"API","title":"JITrench.AutoDiff.Scalar","text":"Scalar{T <: Real} <: Variable\n\nScalar variable.\n\n\n\n\n\n","category":"type"},{"location":"api/#JITrench.AutoDiff.Tensor","page":"API","title":"JITrench.AutoDiff.Tensor","text":"Tensor{T <: AbstractArray} <: AbstractTensor\n\nTensor variable. This Variable is used for CPU.\n\n\n\n\n\n","category":"type"},{"location":"api/#JITrench.AutoDiff.UnaryOperator","page":"API","title":"JITrench.AutoDiff.UnaryOperator","text":"UnaryOperator\n\nDiffableFunction which takes one variable as input.\n\n\n\n\n\n","category":"type"},{"location":"api/#JITrench.AutoDiff._get_gf-Tuple{DiffableFunction}","page":"API","title":"JITrench.AutoDiff._get_gf","text":"_get_gf(f::DiffableFunction)\n\nGet the GradField of the function.\n\n\n\n\n\n","category":"method"},{"location":"api/#JITrench.AutoDiff.backward!-Tuple{Scalar}","page":"API","title":"JITrench.AutoDiff.backward!","text":"backward(y::Scalar; retain_grad = false, create_graph = false)\n\nClculate the gradient for y for all inputs. If retain_grad is true, all intermediate gradients are retained. If create_graph is true, the graph of the backward pass is constructed, allowing to compute higher order derivative products.\n\nArguments\n\ny : Scalar which is the output of the function. Gradient can be immplicitly created only for Scalar.\nretain_grad : If true, all intermediate gradients are retained.\ncreate_graph : If true, the graph of the backward pass is constructed, allowing to compute higher order derivative products.\n\nReturns\n\nnothing: This function returns nothing. The gradient for each input is stored in grad field of each input.\n\nExample\n\njulia> x = Scalar(3)\nScalar{Int64}(3)\n\njulia> y = Scalar(2.0)\nScalar{Float64}(2.0)\n\njulia> z = x * 2y\nScalar{Float64}(12.0)\n\njulia> backward!(z)\n\njulia> x.grad\n4.0\n\njulia> y.grad\n6.0\n\n\n\n\n\n","category":"method"},{"location":"api/#JITrench.AutoDiff.backward-Tuple{DiffableFunction, Any}","page":"API","title":"JITrench.AutoDiff.backward","text":"backward(f::DiffableFunction, gy)\n\nDefinition of backward calculation of F. This function must be implemented for each DiffableFunction.\n\nArguments\n\nF::DiffableFunction : Instance of DiffableFunction which is called.\ngy : propageted gradient from output.\n\nReturns\n\ngxs... : propageted gradient to inputs.\n\nExample\n\njulia>julia> JITrench.AutoDiff.backward(f, 1)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"api/#JITrench.AutoDiff.call!","page":"API","title":"JITrench.AutoDiff.call!","text":"call!(F::Type{<:DiffableFunction}, xs...::Variable)\n\nCall JITrench function with xs... as inputs.\n\n\n\n\n\n","category":"function"},{"location":"api/#JITrench.AutoDiff.call!-Tuple{Type{<:DiffableFunction}, Vararg{JITrench.AutoDiff.Variable}}","page":"API","title":"JITrench.AutoDiff.call!","text":"call!(F::Type{<:DiffableFunction}, xs...::Variable)\n\nCall JITrench function with xs... as inputs.\n\nArguments\n\nF::Type{<:DiffableFunction} : Type of DiffableFunction which is called.\nxs...::Variable : Variable which is passed to F.\n\nReturns\n\ny : values of output.\n\nExample\n\njulia> x = Scalar(3)\nScalar{Int64}(3)\n\njulia> y = Scalar(4)\nScalar{Int64}(4)\n\njulia> JITrench.AutoDiff.call!(JITrench.Add, x, y)\nScalar{Int64}(7)\n\n\n\n\n\n","category":"method"},{"location":"api/#JITrench.AutoDiff.call!-Tuple{Type{<:JITrench.AutoDiff.UnaryOperator}, JITrench.AutoDiff.AdditionalField, JITrench.AutoDiff.Variable}","page":"API","title":"JITrench.AutoDiff.call!","text":"call!(F::Type{<:DiffableFunction}, additional_field::AdditionalField, xs...::Variable)\n\nCall JITrench function with xs... as inputs, and additional_field as optional argument.\n\nArguments\n\nF::Type{<:DiffableFunction} : Type of DiffableFunction which is called.\nadditional_field::AdditionalField : AdditionalField which is passed to F.\nxs...::Variable : Variable which is passed to F.\n\nReturns\n\ny : values of output.\n\nExample\n\njulia> x = Scalar(3)\nScalar{Int64}(3)\n\njulia> y = JITrench.AutoDiff.call!(JITrench.Pow, JITrench.PowField(3), x)\nScalar{Int64}(27)\n\njulia> y = JITrench.AutoDiff.call!(JITrench.Pow, JITrench.PowField(2), x)\nScalar{Int64}(9)\n\n\n\n\n\n","category":"method"},{"location":"api/#JITrench.AutoDiff.check_same_device-Union{Tuple{T}, Tuple{T, T}} where T<:JITrench.AutoDiff.Device","page":"API","title":"JITrench.AutoDiff.check_same_device","text":"check_same_device(device1::Device, device2::Device)\n\nCheck if device1 and device2 are the same device.  If they are the same device, return nothing. Otherwise, throw NotSameDeviceError.\n\nArguments\n\ndevice1: Device to be compared.\ndevice2: Device to be compared.\n\nExample\n\njulia> device1 = JITrench.CPU()\nJITrench.AutoDiff.CPU()\n\njulia> device2 = JITrench.GPU(0)\nJITrench.AutoDiff.GPU(0)\n\njulia> JITrench.check_same_device(device1, device2)\nERROR: All arguments must be in the same device. Arguments are on different GPUs.\n\n\n\n\n\n","category":"method"},{"location":"api/#JITrench.AutoDiff.forward-Tuple{Type{DiffableFunction}, Vararg{Any}}","page":"API","title":"JITrench.AutoDiff.forward","text":"forward(F::Type{DiffableFunction}, args...)\n\nDefinition of forward calculation of F. This function must be implemented for each DiffableFunction. This function is called by call! function with values of inputs.\n\nArguments\n\nF::Type{DiffableFunction} : Type of DiffableFunction which is called.\nargs... : values of inputs.\n\nReturns\n\ny : values of output.\n\nExample\n\njulia>julia> JITrench.AutoDiff.forward(JITrench.Add, 1, 2)\n3\n\n\n\n\n\n","category":"method"},{"location":"api/#JITrench.AutoDiff.get_values-Tuple{T} where T<:JITrench.AutoDiff.Variable","page":"API","title":"JITrench.AutoDiff.get_values","text":"get_values(x)\n\nGet values from x. If x is Variable, return x.values. Otherwise, return x.\n\n\n\n\n\n","category":"method"},{"location":"api/#JITrench.AutoDiff.make_func","page":"API","title":"JITrench.AutoDiff.make_func","text":"make_func(::Type{F}, y, inputs, gen)\n\nCreate DiffableFunction instance from y, inputs and gen.\n\n\n\n\n\n","category":"function"},{"location":"api/#JITrench.AutoDiff.make_func-Union{Tuple{F}, Tuple{Type{F}, Any, Any, Any}} where F<:DiffableFunction","page":"API","title":"JITrench.AutoDiff.make_func","text":"make_func(::Type{F}, y, inputs, gen)\n\nCreate DiffableFunction instance from  y, inputs and gen.\n\nArguments\n\nF::Type{F} : Type of DiffableFunction which is created.\ny : values of output.\ninputs : inputs of F.\ngen : generation of output.\n\n\n\n\n\n","category":"method"},{"location":"api/#JITrench.AutoDiff.make_func-Union{Tuple{F}, Tuple{Type{F}, Vararg{Any, 4}}} where F<:DiffableFunction","page":"API","title":"JITrench.AutoDiff.make_func","text":"make_func(::Type{F}, additional_field, y, inputs, gen)\n\nCreate DiffableFunction instance from additional_field, y, inputs and gen.\n\nArguments\n\nF::Type{F} : Type of DiffableFunction which is created.\nadditional_field : additional_field for F.\ny : values of output.\ninputs : inputs of F.\ngen : generation of output.\n\n\n\n\n\n","category":"method"},{"location":"api/#JITrench.AutoDiff.make_func-Union{Tuple{F}, Tuple{T}, Tuple{Type{F}, T, Any, Any, Any}} where {T<:CUDA.CuArray, F<:DiffableFunction}","page":"API","title":"JITrench.AutoDiff.make_func","text":"make_func(::Type{F}, y::CuArray, inputs, gen, device_idx)\n\nCreate DiffableFunction instance from  y,  inputs , gen and device_idx.\n\nArguments\n\nF::Type{F} : Type of DiffableFunction which is created.\ny : values of output which is CuArray.\ninputs : inputs of F.\ngen : generation of output.\n\n\n\n\n\n","category":"method"},{"location":"api/#JITrench.AutoDiff.out_to_tensor","page":"API","title":"JITrench.AutoDiff.out_to_tensor","text":"out_to_tensor(y, generation; creator=nothing, grad=nothing, req_broadcast=false)\n\nConvert output of forward function to appropriate Variable. (e.g. Scalar, Tensor, CuTensor)\n\nArguments\n\ny : values of output.\ngeneration : generation of output.\ncreator : creator of output.\ngrad : grad of output.\nreq_broadcast : req_broadcast of output.\n\nReturns\n\nout : Variable which is converted from y.   \n\n\n\n\n\n","category":"function"},{"location":"api/#JITrench.AutoDiff.@diffable-Tuple{Any}","page":"API","title":"JITrench.AutoDiff.@diffable","text":"@diffable\n\nCheck definition of DiffableFunction has grad_field field and it's type is GradField.\n\n\n\n\n\n","category":"macro"},{"location":"api/#JITrench.AutoDiff.@nograd-Tuple{Any}","page":"API","title":"JITrench.AutoDiff.@nograd","text":"@nograd\n\nDisable gradient calculation in the scope of the macro.\n\nExample\n\njulia> x = Tensor([1, 2, 3])\n3-element Tensor{Vector{Int64}}:\n 1\n 2\n 3\n\njulia> y = 2x\n3-element Tensor{Vector{Int64}}:\n 2\n 4\n 6\n\njulia> JITrench.AutoDiff.@nograd y = 2x\n3-element Vector{Int64}:\n 2\n 4\n 6\n\njulia> JITrench.AutoDiff.@nograd begin\n           y = 2x\n       end\n3-element Vector{Int64}:\n 2\n 4\n 6\n\n\n\n\n\n","category":"macro"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"JITrench.plot_graph \nJITrench.PNGContainer","category":"page"},{"location":"api/#JITrench.plot_graph","page":"API","title":"JITrench.plot_graph","text":"plot_graph(var :: Variable; to_file=\"\", title=\"\")\n\nDraw a computational graph with y as the end. This requires the installation of graphviz.\n\nArguments\n\nto_file: Specifies the file name where the image will be saved.\n\nIf it is an empty string, it will return an image of type PNGContainer. See also: PNGContainer\n\ntitle: Title of graph.\n\njulia> x = Variable(10)\nname: nothing \nvalues: 10\ncreator: User-Defined(nothing)\n\njulia> y = x + 3\nname: nothing \nvalues: 13\ncreator: JITrench.Add\n\njulia> JITrench.plot_graph(y, to_file=\"graph.png\") \n\n\n\n\n\n","category":"function"},{"location":"api/#JITrench.PNGContainer","page":"API","title":"JITrench.PNGContainer","text":"PNGContainer\n\nA structure for handling images. You can display an image by using it as a return value of a cell or by explicitly display( :: PNGContainer) in Jupyter.\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = JITrench","category":"page"},{"location":"#JITrench","page":"Home","title":"JITrench","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"JITrench.jl is Lightweight Automatic Differentiation & DeepLearning Framework implemented in pure Julia.","category":"page"},{"location":"#Quick-Tour","page":"Home","title":"Quick Tour","text":"","category":"section"},{"location":"#Automatic-Differentiation","page":"Home","title":"Automatic Differentiation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> using JITrench\n\njulia> f(x) = sin(x) + 1\nf (generic function with 1 method)\n\njulia> JITrench.@diff! f(x)\nf′ (generic function with 1 method)\n\njulia> f′(π)\n-1.0","category":"page"},{"location":"#Train-Neural-Network","page":"Home","title":"Train Neural Network","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using JITrench\nusing JITrench.NN \nusing Printf\n\n\nN = 100\np = 1\nn_iter = 20000\n\nx = rand(N, p)\ny = sin.(2π .* x) + (rand(N, p) / 1)\n\nfunction model(x)\n    x = NN.Linear(out_dim=10)(x)\n    x = NN.functions.sigmoid.(x)\n    x = NN.Linear(out_dim=1)(x)\n    return NN.result(x)\nend\n\nparams = NN.init(model, NN.Initializer((nothing, 1)))\noptimizer = NN.SGD(params, 1e-1)\n\nx = Tensor(x)\ny = Tensor(y)\n\nfor iter in 1:n_iter\n    pred = NN.apply(model, x, params)\n    loss = NN.functions.mean_squared_error(y, pred)\n    NN.cleargrads!(params)\n    backward!(loss)\n    NN.optimize!(optimizer)\n    if (iter % 500 == 0)\n        @printf \"[iters] %4i [loss] %.4f\\n\" iter loss.values \n    end\nend\n\n\nNN.save_weight(params, \"weight\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"}]
}
